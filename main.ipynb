{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import mindspore\n",
    "import pandas as pd\n",
    "from mindnlp.transformers import BertForSequenceClassification, BertModel, BertTokenizer\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn, context\n",
    "from mindnlp._legacy.engine import Trainer, Evaluator\n",
    "from mindnlp._legacy.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp._legacy.metrics import Accuracy\n",
    "#导入对应的包"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:17:00.973395Z",
     "start_time": "2024-07-11T15:17:00.968129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据预处理\n",
    "class SentimentDataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self._labels, self._text_a = [], []\n",
    "        self._load()\n",
    "\n",
    "    def load_accusation_id_map(self, filename):\n",
    "        # 使用pandas读取CSV文件\n",
    "        df = pd.read_csv(filename)\n",
    "        # 将DataFrame转换为字典，其中'Accusation'列作为键，'id'列作为值\n",
    "        accusation_id_map = df.set_index('Accusation')['id'].to_dict()\n",
    "        return accusation_id_map\n",
    "\n",
    "    def _load(self):\n",
    "        labelss = self.load_accusation_id_map('D:/pycharm-workspace/Mind/data/label.csv')\n",
    "        with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if case['meta']['accusation'][0] in labelss:\n",
    "                    case = json.loads(line.strip())  # 解析每一行作为一个单独的JSON对象\n",
    "                    label = labelss[case['meta']['accusation'][0]]\n",
    "                    text_a = case['fact']\n",
    "                    self._labels.append(label)\n",
    "                    self._text_a.append(text_a)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._labels[index], self._text_a[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "def process_dataset(source, tokenizer, max_seq_len=64, batch_size=64, shuffle=True):\n",
    "    is_ascend = mindspore.get_context('device_target') == 'gpu'\n",
    "\n",
    "    column_names = [\"label\", \"text_a\"]\n",
    "\n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "    type_cast_op = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    def tokenize_and_pad(text):\n",
    "        if is_ascend:\n",
    "            tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "        else:\n",
    "            # tokenized = tokenizer(text)\n",
    "            tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "        return tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "    # map dataset\n",
    "    dataset = dataset.map(operations=tokenize_and_pad, input_columns=\"text_a\",\n",
    "                          output_columns=['input_ids', 'attention_mask'])\n",
    "    dataset = dataset.map(operations=[type_cast_op], input_columns=\"label\", output_columns='labels')\n",
    "    # batch dataset\n",
    "    if is_ascend:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.padded_batch(batch_size, pad_info={'input_ids': (None, tokenizer.pad_token_id),\n",
    "                                                             'attention_mask': (None, 0)})\n",
    "\n",
    "    return dataset"
   ],
   "id": "dc9805ee4ebf0da4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#加载模型和定义参数\n",
    "tokenizer = BertTokenizer.from_pretrained('../pretrained/')\n",
    "# set bert config and define parameters for training\n",
    "model = BertForSequenceClassification.from_pretrained('../pretrained/', num_labels=134)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "# optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-4)\n",
    "optimizer = nn.AdaMax(model.trainable_params(), learning_rate=2e-4)\n",
    "\n",
    "metric = Accuracy()\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='bert_emotect', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='bert_emotect_best', auto_load=True)\n"
   ],
   "id": "452bbadbc2261fd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#加载数据\n",
    "dataset_train = process_dataset(\n",
    "    SentimentDataset('E:/Data/CAIL2018_ALL_DATA/final_all_data/exercise_contest/data_train.json'), tokenizer)\n",
    "dataset_val = process_dataset(\n",
    "    SentimentDataset(\"E:/Data/CAIL2018_ALL_DATA/final_all_data/exercise_contest/data_valid.json\"), tokenizer)\n",
    "dataset_test = process_dataset(\n",
    "    SentimentDataset(\"E:/Data/CAIL2018_ALL_DATA/final_all_data/exercise_contest/data_test.json\"), tokenizer,\n",
    "    shuffle=False)"
   ],
   "id": "a035aabcb6d51cbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#定义训练函数并训练\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=5, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb])\n",
    "\n",
    "trainer.run(tgt_columns=\"labels\")"
   ],
   "id": "a284642cf5518796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#加载模型进行测试\n",
    "metric = Accuracy()\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='bert_emotect', epochs=1, keep_checkpoint_max=203)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='bert_emotect_best', auto_load=True)\n",
    "\n",
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"labels\")"
   ],
   "id": "59b49924f7d9727d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#模型推理\n",
    "def predict(text, label=None):\n",
    "    global labelsss\n",
    "    labelsss = reversed_dict = {v: k for k, v in labelsss.items()}\n",
    "    label_map = labelsss\n",
    "\n",
    "    text_tokenized = Tensor([tokenizer(text).input_ids])\n",
    "    logits = model(text_tokenized)\n",
    "    predict_label = logits[0].asnumpy().argmax()\n",
    "    info = f\"inputs: '{text}', predict: '{label_map[predict_label]}'\"\n",
    "    if label is not None:\n",
    "        info += f\" , label: '{label_map[label]}'\"\n",
    "    print(info)\n",
    "\n",
    "predict(\"公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，\"\n",
    "        \"其行为已触犯《中华人民共和国刑法》××之规定，应当以××罪追究其刑事责任。建议判处被告人张某某××以下刑罚，并处罚金。\")\n"
   ],
   "id": "6c9bc382910d6da3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
